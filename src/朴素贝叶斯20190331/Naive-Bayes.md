#朴素贝叶斯学习
本来这周要看下怎么优化kmeans方法计算yolo预选框的，但是考虑到这个问题并不紧急，所以暂时先放一放。然后后面要做的事情中都会遇到文本分类的问题，所以借每周学习的这么点机会，了解一些文本分类方面的知识，然后具体的算法的话就是用朴素贝叶斯的方法，对从看点数据库拿到的两类文本进行训练和验证。
##文本分类
在了解朴素贝叶斯方法之前，先来了解一下文本分类的知识。

文本分类是在预定义的分类体系下，根据文本的特征（内容或属性），将给定文本与一个或多个类别相关联的过程。因此，文本分类研究涉及文本内容理解和模式分类等若干自然语言理解和模式识别问题。

文本分类分类以下几个方面：文本表示，特征权重计算方法，分类器设计，文本分类评测指标。而文本表示又分为，文本向量化，向量的相似度量和文本特征选择方法。

一、**文本表示**

一个文本表现为一个由文字和标点符号组成的字符串，由字或字符组成词，由词组成短语，进而形成句、段、节、章、篇的结构。目前文本表示通常采用向量空间模型（vecto rspace model,VSM）。简单的说就是把文档呈现的词、句子、段落等形式都转变成特征项的不同权重，给不同特征不同的权重，最终得到文章就可以用向量来表示。

二、**向量的相似性度量**

向量的相似性的概念就是两个向量之间的某种距离，用这个距离来表示文档的相似系数，通常都是用內积来表示。

$$ Sim(D_1,D_2) = \sum_{k=1}^{n} \omega_{1k}\times\omega_{1k} ​$$

如果还要考虑到归一化，则使用两个向量夹角的余弦值来表示相似系数：

$$ Sim(D_1,D_2) = \cos\theta = \frac{\sum_{k=1}^{n} \omega_{1k}\times\omega_{1k} }{\sqrt[][\sum_{k=1}^{n}\omega_{1k}^2\sum_{k=1}^{n}\omega_{2k}^2]} $$

三、**文本特征选择方法**

在向量空间模型中，表示文本的特征项可以选择字、词、短语，甚至"概念"等多种元素。特征选择的方法很多，常用的有基于文档频率、信息增益法、$$\chi^2$$统计量和互信息方法等。今天就只重点看下基于文档频率的特征提取法。

文档频率是指出现某个特征项的文档的频率。基于文档频率的特征提取法通常是：从训练语料中统计出包含某个特征的文档的频率（个数），然后根据设定的阈值，当该特征项的DF值小于某个阈值时，从特征空间中去掉该特征项，因为该特征项使文档出现的频率太低，没有代表性；当该特征项的DF值大于另外一个阈值时，从特征空间中也去掉该特征项，因为该特征项使文档出现的频率太高，没有区分度。

基于文档频率的特征选择方法可以**降低向量计算的复杂度，并可能提高分类的准确率**，因为按这种选择方法可以**去掉一部分噪声特征**。这种方法简单、易行。但严格地讲，这种方法只是一种借用算法，其理论根据不足。根据信息论我们知道，**某些特征虽然出现频率低，但往往包含较多的信息**，对于分类的重要性很大。对于这类特征就**不应该使用DF方法**将其直接排除在向量特征之外。

四、特征权重计算方法

介绍几个常用的特征权重计算方法：

1. 布尔权重：如果文中出现了该特征，则向量中该分量为1，否则为0.
2. 绝对词频(TF)：使用特征项在文本中出现的频度表示文本
3. 倒排文档频度：稀有特征比常用特征含有更新（我觉得这里是更多才对吧）的信息
4. tf-idf：$$\omega_{ij}=tf_{ij}\times\log\frac{N}{n_i}$$,权重与特征项在文档中出现的频率成正比，与在整个语料中出现该特征项的文档数成反比

## 朴素贝叶斯

进入正题。

假设文本是基于词的一元模型，即文本中当前词的出现依赖于文本类别，但不依赖于其他词及文本的长度，也就是说，词与词之间是独立的。根据贝叶斯公式，文档Doc属于Ci类的概率为 

$$ P(C_i|Doc)= \frac{P(Doc|C_i)\times P(C_i)}{P(Doc)} $$

在具体实现时，通常又分为两种情况：
（1）文档Doc采用DF向量表示法，即文档向量V的分量为一个布尔值，0表示相应的特征在该文档中未出现，1表示特征在文档中出现。
（2）若文档Doc采用TF向量表示法，即文档向量V的分量为相应特征在该文档中出现的频度。